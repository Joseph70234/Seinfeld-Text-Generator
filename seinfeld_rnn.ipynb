{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dir\n",
    "os.chdir('C:/Users/theon/OneDrive/Desktop/Giga Projects/seinfeld text gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 4767047 characters\n"
     ]
    }
   ],
   "source": [
    "# open text\n",
    "text = open('standardized_seinfeld_script.txt', 'rb').read().decode(encoding='utf-8')       # 'rb' specifies to read it in binary mode (basically manual handling of info, not automatic)\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Seinfeld Chronicles\n",
      "\n",
      "[SCENE: Comedy club]\n",
      "\n",
      "JERRY: You know, why we're here? [he means: here in the \"Comedy club\"] To be out, this is out...and out is one of the single most enjoyable experiences of life. People...did you ever hear people talk\n"
     ]
    }
   ],
   "source": [
    "# check first 250 chars\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 unique characters\n"
     ]
    }
   ],
   "source": [
    "# check unique chars\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into characters\n",
    "chars = tf.strings.unicode_split(text, input_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map chars to ints\n",
    "ids_from_chars = tf.keras.layers.StringLookup(                      # function for converting chars to integer indices\n",
    "    vocabulary=list(vocab),                                         # tell it how many vocab items to map\n",
    "    mask_token=None)                                                # this specifies what to use when padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map ints back to chars\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(),                     # get vocabulary from above ids_from_chars object\n",
    "    invert=True,                                                    # do it the other way, lol\n",
    "    mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to return strings from integers\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply map\n",
    "all_ids = ids_from_chars(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from all_ids tensors\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences from dataset\n",
    "seq_length = 100\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)        # seq_length+1 is to have both inputs (100) and targets (the last 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for extracting input and target\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply sequence splitter function to dataset\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b\"The Seinfeld Chronicles\\r\\n\\r\\n[SCENE: Comedy club]\\r\\n\\r\\nJERRY: You know, why we're here? [he means: here \"\n",
      "Target: b\"he Seinfeld Chronicles\\r\\n\\r\\n[SCENE: Comedy club]\\r\\n\\r\\nJERRY: You know, why we're here? [he means: here i\"\n"
     ]
    }
   ],
   "source": [
    "# check first input and target pair\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set processing options\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# final processing\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))               # prefetch for efficiency (tf.data.experimental.AUTOTUNE is to set prefetch size automatically/dynamically for efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(ids_from_chars.get_vocabulary())         # amount of unique characters in input\n",
    "embedding_dim = 256                                       # embedding is representing tokens as n-dimensional vectors (understood better by models); here, each token is represented by 256-dimension vector\n",
    "rnn_units = 1024                                          # number of units in GRU layer; more units, larger capacity to capture patterns in sequential data\n",
    "\n",
    "# define custom RNN model, inheriting from tf.keras.Model\n",
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)         # embed tokenized chars\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './sein_rnn/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "737/737 [==============================] - 53s 69ms/step - loss: 1.2522\n",
      "Epoch 2/30\n",
      "737/737 [==============================] - 52s 68ms/step - loss: 1.1718\n",
      "Epoch 3/30\n",
      "737/737 [==============================] - 51s 68ms/step - loss: 1.1176\n",
      "Epoch 4/30\n",
      "737/737 [==============================] - 52s 68ms/step - loss: 1.0741\n",
      "Epoch 5/30\n",
      "737/737 [==============================] - 52s 68ms/step - loss: 1.0372\n",
      "Epoch 6/30\n",
      "737/737 [==============================] - 52s 68ms/step - loss: 1.0049\n",
      "Epoch 7/30\n",
      "737/737 [==============================] - 52s 68ms/step - loss: 0.9768\n",
      "Epoch 8/30\n",
      "737/737 [==============================] - 52s 68ms/step - loss: 0.9538\n",
      "Epoch 9/30\n",
      "737/737 [==============================] - 52s 68ms/step - loss: 0.9339\n",
      "Epoch 10/30\n",
      "737/737 [==============================] - 52s 68ms/step - loss: 0.9190\n",
      "Epoch 11/30\n",
      "737/737 [==============================] - 120s 161ms/step - loss: 0.9067\n",
      "Epoch 12/30\n",
      "737/737 [==============================] - 52s 69ms/step - loss: 0.8992\n",
      "Epoch 13/30\n",
      "737/737 [==============================] - 52s 68ms/step - loss: 0.8933\n",
      "Epoch 14/30\n",
      "737/737 [==============================] - 51s 67ms/step - loss: 0.8900\n",
      "Epoch 15/30\n",
      "737/737 [==============================] - 303s 409ms/step - loss: 0.8877\n",
      "Epoch 16/30\n",
      "737/737 [==============================] - 52s 69ms/step - loss: 0.8879\n",
      "Epoch 17/30\n",
      "737/737 [==============================] - 52s 68ms/step - loss: 0.8884\n",
      "Epoch 18/30\n",
      "737/737 [==============================] - 2828s 4s/step - loss: 0.8901\n",
      "Epoch 19/30\n",
      "737/737 [==============================] - 50s 66ms/step - loss: 0.8954\n",
      "Epoch 20/30\n",
      "737/737 [==============================] - 51s 67ms/step - loss: 0.8985\n",
      "Epoch 21/30\n",
      "737/737 [==============================] - 52s 68ms/step - loss: 0.9058\n",
      "Epoch 22/30\n",
      "737/737 [==============================] - 51s 67ms/step - loss: 0.9129\n",
      "Epoch 23/30\n",
      "737/737 [==============================] - 51s 67ms/step - loss: 0.9195\n",
      "Epoch 24/30\n",
      "737/737 [==============================] - 51s 67ms/step - loss: 0.9260\n",
      "Epoch 25/30\n",
      "737/737 [==============================] - 41s 53ms/step - loss: 0.9350\n",
      "Epoch 26/30\n",
      "737/737 [==============================] - 35s 46ms/step - loss: 0.9496\n",
      "Epoch 27/30\n",
      "737/737 [==============================] - 35s 46ms/step - loss: 0.9620\n",
      "Epoch 28/30\n",
      "737/737 [==============================] - 35s 46ms/step - loss: 0.9684\n",
      "Epoch 29/30\n",
      "737/737 [==============================] - 35s 46ms/step - loss: 0.9860\n",
      "Epoch 30/30\n",
      "737/737 [==============================] - 35s 46ms/step - loss: 1.0030\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JERRY: I'm the worst. The physical crime he's in her seat. Elaine: Hey. Jerry: hey I wanna be a par and an anitortuge can read.\n",
      "JERRY: (Pretty mediface. I just sit eating anything along, though wrongholed, just as the carpet cleanal school onhers exchange money. JERRY: Sheed we played the other theater lately shoulder, did you been paying your New Yarks? That's some guy that consider. That defensively's autression would posted out first. Jerry grabs his arms on tope of carrying machine, she sees my foot. GEORGE: Get a good towel. KRAMER: (pointing) Mrs Hamostup. My setting is of way I know I\n",
      "was in gimageea authorad. It was 2..\n",
      "JERRY: I'll think we're trying to get something. So what's wrong with?\n",
      "CINDY: Yes, he's a drip. It's business.\n",
      "GEORGE: You gotta meet you out. Written with my egms on Morty's rye. (Frantigan offers humiliatiquit\n",
      "over his shirt 'craceler, sequence inocks highs, making her bag shorting to put you two hips around. So, matter what happened to the calk for my allermo \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.9638869762420654\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['JERRY:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
